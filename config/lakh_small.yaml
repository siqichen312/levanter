data:
  train_urls:
    - "/user/s/siqichen/projects/amt/anticipation/data/train.txt"
  validation_urls:
    - "/user/s/siqichen/projects/amt/anticipation/data/valid.txt"
  
  cache_dir: "/user/s/siqichen/projects/amt/anticipation/data/cache/"
  
  # Use passthrough tokenizer for pre-tokenized data
  tokenizer: "passthrough"
  vocab_size: 55028  # Required for passthrough tokenizer
  enforce_eos: false

model:
  type: gpt2
  hidden_dim: 768
  num_heads: 12
  num_layers: 12
  seq_len: 1024
  scale_attn_by_inverse_layer_idx: true

trainer:
  mp: p=f32,c=bfloat16
  model_axis_size: 1
  per_device_parallelism: 4
  train_batch_size: 512

  checkpointer:
    base_path: "/user/s/siqichen/projects/amt/anticipation/checkpoints/"
    save_interval: 30m

  axis_resources:
    batch: "data"
    vocab: "model"
    mlp: "model"
    heads: "model"
  parameter_axis_resources:
    embed: "data"

optimizer:
  learning_rate: 6E-4
  weight_decay: 0.1
